0P 
Good evening, everyone. I'm Ryuichi Ikeya from Institute of Science Tokyo. 
Today, I'd like to introduce a new method for estimating force from deformation. 

1P 

First, let’s start with the technical background. 
Recently, it has become possible to easily extract the shape of an object. 

In terms of software development, classification of object at pixel level is now possible. 
For example, Meta released their latest segmentation model, which makes it easy to extract shape feature from images. 

In terms of hardware development, capturing 3D images using inexpensive RGBD cameras is now possible. 
(With advances in software and hardware, it is now possible to obtain the 3D data of an object by extracting the object from a depth image.) 

These technical advancements open a new way of estimating force only by observing deformations 

2P
In other words, conventionally, force was measured by embedding sensors like this. 

3P 
In the future, we will be able to measure forces by observing deformation like this. 
(そしてこの、変形観察による力計測は、生活環境のモニタリンによる人間のセンシングや埋め込みの必要がないロボットの触覚センサとしての利用など、従来の力センサでは実現が難しい様々な応用が期待されます。) 

4P 
Vision sensors that estimate forces from images are attracting attention, and there is some related research. 
In particular, by using machine learning based on experimental data, models can be easily created and inference can be performed in real time. 
However, existing research has some limitations because it simply outputs a scalar from a plain RGB image. 
First, it is not flexible to changes in the experimental environment. For example, changes in background, changes in lighting, presence of humans. 
Second, just single-point force estimation limits application possibilities. For example, ... 

5P
So, in this study, we propose a new force sensor principle that estimates force distribution only by observing shape deformation information and verifying its feasibility. 
In the proposed method, we first capture video, then extract shape deformation, and finally output image format force using a convolutional neural network. 
The proposed method has two novelties. 
First, by extracting shape deformation information, it is flexible to environmental changes. 
Secondly, by expressing force in image format, multi-point estimation rather than single-point estimation become possible. 

6P
This slide shows the details of the proposed method.
In the inference process, shape deformation information is extracted from two images, one before and one after deformation, and force distribution is inferred using a trained machine learning model (CNN)

7P
In the training process, shape deformation information is extracted from the depth and RGB image by image processing, and the force distribution can be expressed in image format by embedding the sensor values.
By learning the relationship between this shape deformation information and force distribution, we can create a model and infer force distribution from an RGBD camera.

8P
Next, we explain how to represent shape deformation information.
The shape deformation information is composed of three elements as images: the shape before deformation, the shape after deformation, and the contact position.
In addition, we prepared contour and depth information for the shape before deformation and after deformation.
The contour information is a binary image, and the depth information is a grayscale image normalized to 0 to 1. 

9P
The architecture of the machine learning model is the typical encoder-decoder model which is used for tasks with image inputs and output.
As a unique innovation of this research, we introduced a mechanism to limits the model output to a reasonable range depending on the contact position. 

10P
Next, I will explain how to create a dataset.
This consists of two steps: extracting shape deformation information as input data, and creating force distribution images as correct answer data.
First, I will explain the process of extracting shape deformation information using image processing. 

11P
This slide shows an overview of extraction of shape deformation information by image processing.
First, we extract the binary image of the contour by clustering the RGB image.
Next, the depth image of the object is extracted by multiplying the depth image and the binary image of the contour.
In this process, by extracting the shape information of the object, the model becomes robust against background changes.
Finally, for contact position, we use a skeleton estimation method called openpose, and obtain a binary image of 11 representative points on the hand.
To do so, we can extract contact information.

12P
Next, we will explain how to create teacher data using force sensor information. 

13P
To create the teacher data, first, we use openpose to identify 11 representative finger skeleton coordinates.
Next, the force sensor values are assigned to the corresponding pixels to create the force distribution in image format.
Finally, a force distribution image like this figure can be obtained for each measurement time. 

14P
Until now, force distribution has been expressed in two-dimensional image format, but two-dimensional images have poor visibility for humans to understand.
Therefore, in this research, we developed a system that displays force distribution as a vector on a three-dimensional point cloud.
This is achieved by assuming that the force is applied perpendicularly to the object at each position of the force distribution.
Using this system, experimental results are displayed as a 3D point cloud rather than as an image. 

15P
This is the numerical result.
As you can see in the table, the MRE for the test data was 19%.
The error of 19% is a bit higher compared to super precise sensors like load cells.
However, it's within the acceptable margin for pressure sensors. 